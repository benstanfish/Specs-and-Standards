{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b44bca",
   "metadata": {},
   "source": [
    "# JES Standards Scrape with BeautifulSoup (v2)\n",
    "\n",
    "This script parses .sec files (which are actually XML), and produces an Excel report with cited standards by JES section.\n",
    "\n",
    "Jupyter Notebook written by Ben Fisher on 25 November 2024 <br>\n",
    "**benjamin.s.fisher@usace.army.mil**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2ebd0",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The following imports are assumed to have been previously installed (for Notebook installs, use *! pip install ~*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2cd2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, datetime, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import lxml\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981c9d9",
   "metadata": {},
   "source": [
    "##### Warning Suppression (Jupyter Notebooks only)\n",
    "\n",
    "The following code will suppress the user warning generated by Beautiful Soup when parsing XML files with lxml: *XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?)...*\n",
    "\n",
    "This is necessary **only** when running the script in a Notebook context. You can skip this line if script ran elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ce7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95150ade",
   "metadata": {},
   "source": [
    "### Path Definitions\n",
    "\n",
    "Assumptions:\n",
    "- Batch processing will be assumed. Tkinter's filedialog.askdirectory() won't be used, rather, it'll be assumed that files for processing are placed in named subfolders, adjacent to this .ipynb (Notebook) file.\n",
    "- Files will be .sec (internally are XML).\n",
    "- Div 00 will be assumed not included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86068b15",
   "metadata": {},
   "source": [
    "##### Directories\n",
    "Working directories are made relative to the 'current working directory,' which is where the Notebook (.ipynb) file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a76eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = os.getcwd()\n",
    "\n",
    "jes_masters = parent_folder + '\\\\JES\\\\'\n",
    "ufgs_masters = parent_folder + '\\\\UFGS\\\\'\n",
    "\n",
    "jes_cleaned = parent_folder + '\\\\JES Cleaned\\\\'\n",
    "ufgs_cleaned = parent_folder + '\\\\UFGS Cleaned\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465ffc4",
   "metadata": {},
   "source": [
    "### Clean 0x81 Character from Files\n",
    "The non-printing byte 0x81 (129) creates an ill-formed error. Simply opening the file as a UTF-8 string is problematic, so read the string as bytes, then replace the character and write as bytes.\n",
    "\n",
    "Don't use BS4 for this, rather read the binary.\n",
    "\n",
    "I believe this may be the result of copy-pasting standards titles from the the web: non-printing characters are accidentally introduced?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11116e88",
   "metadata": {},
   "source": [
    "##### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_files(folder):\n",
    "    find_text = b'\\x81'\n",
    "    replace_text = b''\n",
    "    error_files = []\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        file_path = folder + file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "        start_count = len(content)\n",
    "        content = content.replace(find_text, replace_text)\n",
    "        end_count = len(content)   \n",
    "\n",
    "        if not end_count == start_count:\n",
    "            error_files.append(file)\n",
    "            \n",
    "    return error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b3da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(file):\n",
    "    find_text = b'\\x81'\n",
    "    replace_text = b''\n",
    "    \n",
    "    with open(file, 'rb') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    content = content.replace(find_text, replace_text) \n",
    "\n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b683fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_clean_files(folder, file_list):\n",
    "    for file in file_list:\n",
    "        file_path = folder + file\n",
    "        clean_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376d42e",
   "metadata": {},
   "source": [
    "##### Get UFGS Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3464c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02 83 00.SEC']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufgs_error_files = get_error_files(ufgs_masters)\n",
    "\n",
    "if len(ufgs_error_files) > 0:\n",
    "    ufgs_unicode_error_report = parent_folder + '\\\\UFGS Unicode Byte Report ' + '{:%Y%m%d %H%M%S}'.format(datetime.datetime.now()) + '.xlsx'\n",
    "\n",
    "    df = pd.DataFrame(ufgs_error_files)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.to_excel(ufgs_unicode_error_report, header=['Sections'])\n",
    "\n",
    "ufgs_error_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a70322",
   "metadata": {},
   "source": [
    "##### Get JES Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc2b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02 83 00.sec',\n",
       " '05 12 00.SEC',\n",
       " '05 30 00.SEC',\n",
       " '05 51 33.SEC',\n",
       " '05 52 00.SEC',\n",
       " '07 60 00.SEC',\n",
       " '08 11 16.SEC',\n",
       " '08 31 00.sec',\n",
       " '08 91 00.sec',\n",
       " '10 14 00.10.SEC',\n",
       " '10 14 00.20.SEC',\n",
       " '21 30 00.SEC',\n",
       " '22 00 00.sec',\n",
       " '23 05 15.SEC',\n",
       " '23 07 00.SEC',\n",
       " '23 23 00.SEC',\n",
       " '23 30 00.SEC',\n",
       " '23 64 26.SEC',\n",
       " '23 65 00.SEC',\n",
       " '23 81 00.SEC',\n",
       " '26 11 16.00 33.SEC',\n",
       " '33 11 00.SEC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jes_error_files = get_error_files(jes_masters)\n",
    "\n",
    "if len(jes_error_files) > 0:\n",
    "    jes_unicode_error_report = parent_folder + '\\\\JES Unicode Byte Report ' + '{:%Y%m%d %H%M%S}'.format(datetime.datetime.now()) + '.xlsx'\n",
    "\n",
    "    df = pd.DataFrame(jes_error_files)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.to_excel(jes_unicode_error_report, header=['Sections'])\n",
    "\n",
    "jes_error_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20dd36",
   "metadata": {},
   "source": [
    "##### Clean 0x81 Character in Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb906ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_clean_files(ufgs_cleaned, ufgs_error_files)\n",
    "batch_clean_files(jes_cleaned, jes_error_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f41d9",
   "metadata": {},
   "source": [
    "###### Check if Error Files Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3dcd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check UFGS cleaned: 0 errors\n",
      "Check JES cleaned: 0 errors\n"
     ]
    }
   ],
   "source": [
    "ufgs_cleaned_test = get_error_files(ufgs_cleaned)\n",
    "jes_cleaned_test = get_error_files(jes_cleaned)\n",
    "print(f'Check UFGS cleaned: {len(ufgs_cleaned_test)} errors')\n",
    "print(f'Check JES cleaned: {len(jes_cleaned_test)} errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dafa7c",
   "metadata": {},
   "source": [
    "### Read Files for Unprocessed Changes\n",
    "Read through files to see if there are any \\<ADD\\> or \\<DEL\\> tags. We have to clean out the \\<DEL\\> tags that poison everything, especially because examples of their inconsistent application have been found in several sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fcc267",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb75341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_tags(soup, tag_name):\n",
    "    # This processes the SOUP not the files\n",
    "    tags = soup.find_all(tag_name)\n",
    "    needs_repair = True if len(tags) > 0 else False\n",
    "    for tag in tags:\n",
    "        tag.decompose()\n",
    "    return needs_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a170f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unprocessed_files(folder):\n",
    "    unprocessed_files = []\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        file_path = folder + file\n",
    "        if Path(file_path).suffix.lower() == '.sec':\n",
    "            try:\n",
    "                with open(file_path, 'r') as doc:\n",
    "                    soup = bs.BeautifulSoup(doc.read(), 'lxml')\n",
    "                    if len(soup.find_all('del')) > 0:\n",
    "                        unprocessed_files.append(file)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return unprocessed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa54575",
   "metadata": {},
   "source": [
    "##### Process Cleaned Files (0x81 removed) to Find Unprocessed Files ('\\<ADD\\> and \\<DEL\\> tags remaining')\n",
    "These next two cells are somewhat time consuming because you have to create the Soup for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "094081f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files with <DEL> tags: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprocessed_ufgs = get_unprocessed_files(ufgs_cleaned)\n",
    "\n",
    "print(f'Total files with <DEL> tags: {len(unprocessed_ufgs)}')\n",
    "\n",
    "if len(unprocessed_ufgs) > 0:\n",
    "    ufgs_unprocessed_report = parent_folder + '\\\\UFGS Unprocessed Report ' + '{:%Y%m%d %H%M%S}'.format(datetime.datetime.now()) + '.xlsx'\n",
    "\n",
    "    df = pd.DataFrame(unprocessed_ufgs)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.to_excel(ufgs_unprocessed_report, header=['Sections'])\n",
    "\n",
    "unprocessed_ufgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b10422e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files with <DEL> tags: 106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['01 11 00.00 10.sec',\n",
       " '01 11 30.00 25.sec',\n",
       " '01 32 01.00 10.sec',\n",
       " '01 33 00.sec',\n",
       " '01 33 16.sec',\n",
       " '01 33 29.sec',\n",
       " '01 35 26.sec',\n",
       " '01 42 00.sec',\n",
       " '01 45 00.00 10.sec',\n",
       " '01 45 00.15 10.sec',\n",
       " '01 45 35.SEC',\n",
       " '01 50 00.sec',\n",
       " '01 57 19.01.sec',\n",
       " '01 57 19.sec',\n",
       " '01 74 19.sec',\n",
       " '01 78 00.SEC',\n",
       " '02 61 13.sec',\n",
       " '02 65 00.sec',\n",
       " '02 81 00.sec',\n",
       " '02 82 00.sec',\n",
       " '02 83 00.sec',\n",
       " '02 84 16.sec',\n",
       " '02 84 33.sec',\n",
       " '03 30 00.SEC',\n",
       " '05 05 23.13 10.SEC',\n",
       " '05 05 23.16.SEC',\n",
       " '05 12 00.SEC',\n",
       " '05 30 00.SEC',\n",
       " '05 50 13.SEC',\n",
       " '05 51 00.sec',\n",
       " '05 51 33.SEC',\n",
       " '05 52 00.SEC',\n",
       " '06 10 00.sec',\n",
       " '06 41 16.00 10.SEC',\n",
       " '07 05 23.sec',\n",
       " '07 14 00.SEC',\n",
       " '07 21 16.sec',\n",
       " '07 27 10.00 10.sec',\n",
       " '07 52 00.SEC',\n",
       " '07 60 00.SEC',\n",
       " '07 92 00.SEC',\n",
       " '08 11 13.sec',\n",
       " '08 11 16.SEC',\n",
       " '08 14 00.SEC',\n",
       " '08 31 00.sec',\n",
       " '08 32 13.sec',\n",
       " '08 41 13.sec',\n",
       " '08 51 13.SEC',\n",
       " '08 51 23.sec',\n",
       " '08 71 00.SEC',\n",
       " '08 81 00.SEC',\n",
       " '08 91 00.sec',\n",
       " '09 22 00.SEC',\n",
       " '09 29 00.SEC',\n",
       " '09 30 10.SEC',\n",
       " '09 51 00.SEC',\n",
       " '09 65 00.SEC',\n",
       " '09 68 00.SEC',\n",
       " '09 90 00.SEC',\n",
       " '10 14 00.10.SEC',\n",
       " '10 14 00.20.SEC',\n",
       " '10 21 13.SEC',\n",
       " '10 26 00.sec',\n",
       " '10 28 13.SEC',\n",
       " '12 21 00.SEC',\n",
       " '14 21 23.sec',\n",
       " '21 13 13.sec',\n",
       " '21 30 00.SEC',\n",
       " '22 00 00.sec',\n",
       " '23 05 15.SEC',\n",
       " '23 05 48.19.sec',\n",
       " '23 07 00.SEC',\n",
       " '23 23 00.SEC',\n",
       " '23 30 00.SEC',\n",
       " '23 64 10.SEC',\n",
       " '23 64 26.SEC',\n",
       " '23 65 00.SEC',\n",
       " '23 81 00.SEC',\n",
       " '26 05 48.00 10.sec',\n",
       " '26 11 16.00 33.SEC',\n",
       " '26 12 19.10.SEC',\n",
       " '26 20 00.sec',\n",
       " '26 23 00.SEC',\n",
       " '26 24 13.SEC',\n",
       " '26 29 23.sec',\n",
       " '26 41 00.sec',\n",
       " '26 51 00.SEC',\n",
       " '26 56 00.SEC',\n",
       " '27 10 00.SEC',\n",
       " '31 00 00.SEC',\n",
       " '31 21 13.sec',\n",
       " '31 62 13.20.SEC',\n",
       " '32 11 20.SEC',\n",
       " '32 11 23.SEC',\n",
       " '32 12 13.SEC',\n",
       " '32 12 16.16.SEC',\n",
       " '32 15 00.SEC',\n",
       " '32 16 19.SEC',\n",
       " '32 31 13.SEC',\n",
       " '33 11 00.SEC',\n",
       " '33 30 00.SEC',\n",
       " '33 40 00.SEC',\n",
       " '33 71 02.SEC',\n",
       " '33 82 00.SEC',\n",
       " '34 11 00.sec',\n",
       " '35 59 13.16.sec']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprocessed_jes = get_unprocessed_files(jes_cleaned)\n",
    "\n",
    "print(f'Total files with <DEL> tags: {len(unprocessed_jes)}')\n",
    "\n",
    "if len(unprocessed_jes) > 0:   \n",
    "    jes_unprocessed_report = parent_folder + '\\\\JES Unprocessed Report ' + '{:%Y%m%d %H%M%S}'.format(datetime.datetime.now()) + '.xlsx'\n",
    "\n",
    "    df = pd.DataFrame(unprocessed_jes)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.to_excel(jes_unprocessed_report, header=['Sections'])\n",
    "\n",
    "unprocessed_jes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc811502",
   "metadata": {},
   "source": [
    "### Parse and Process Files\n",
    "The following route is intended to batch parse all files in a folder, add the relevant REF data to a global list, then export to an Excel report.\n",
    "\n",
    "##### Assumptions\n",
    "- The **0x81** character was previously removed from all files\n",
    "- Files may contain \\<DEL\\> tags, which will be processed at the Soup level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d42c32",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24cc4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_titles(title):\n",
    "    pattern = '^\\(.*?\\)\\s*'\n",
    "    x = re.split(pattern, title)\n",
    "    \n",
    "    if len(x) > 1:\n",
    "        # Recover the edition and clean the text\n",
    "        x[0] = re.findall(pattern, title)[0].replace('\\n','').strip()\n",
    "        x[0] = x[0].strip('()')\n",
    "    \n",
    "        # Swap indicies so that edition is the last element\n",
    "        x[0], x[1] = x[1], x[0]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480c457",
   "metadata": {},
   "source": [
    "##### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a802993d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_standards_list(folder):\n",
    "\n",
    "    standards = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        file_path = folder + file\n",
    "        if Path(file_path).suffix.lower() == '.sec':\n",
    "            try:\n",
    "                with open(file_path, 'r') as doc:\n",
    "                    soup = bs.BeautifulSoup(doc.read(), 'lxml')\n",
    "                    kill_tags(soup, 'del')\n",
    "\n",
    "                    section = {\n",
    "                        'number': soup.find('scn').text[8:],\n",
    "                        'name': soup.find('stl').text,\n",
    "                        'date': soup.find('dte').text\n",
    "                    }\n",
    "\n",
    "                    ref_elements = soup.find_all('ref')\n",
    "                    if len(ref_elements) > 0:\n",
    "                        orgs = []\n",
    "                        abbrev_string = '\\(.*?\\)$'\n",
    "                        for ref in ref_elements:\n",
    "                            raw = ref.find('org').text.replace('\\n','').strip()\n",
    "                            parsed = re.split(abbrev_string, raw)\n",
    "                            if len(parsed) > 1:\n",
    "                                parsed[0] = parsed[0].strip()\n",
    "                                parsed[1] = re.findall(abbrev_string, raw)[0].strip('()')\n",
    "                            orgs.append(parsed)\n",
    "\n",
    "                        for i in range(len(ref_elements)):\n",
    "                            org_name = orgs[i][0]\n",
    "                            org_abbrev = orgs[i][1]\n",
    "\n",
    "                            ids = ref_elements[i].find_all('rid')\n",
    "                            titles = ref_elements[i].find_all('rtl')   \n",
    "\n",
    "                            for j in range(len(ids)):\n",
    "                                this_id = ids[j].text.replace('\\n','').strip()\n",
    "                                this_title = titles[j].text.replace('\\n','').strip()\n",
    "                                title = split_titles(this_title)\n",
    "                                if len(title) > 1:\n",
    "                                    standards.append([section['number'], \n",
    "                                                      section['name'], \n",
    "                                                      section['date'], \n",
    "                                                      org_name, \n",
    "                                                      org_abbrev, \n",
    "                                                      this_id, \n",
    "                                                      title[0], \n",
    "                                                      title[1]])\n",
    "                                else:\n",
    "                                    standards.append([section['number'], \n",
    "                                                      section['name'], \n",
    "                                                      section['date'], \n",
    "                                                      org_name, \n",
    "                                                      org_abbrev, \n",
    "                                                      this_id, \n",
    "                                                      title[0], \n",
    "                                                      ''])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return standards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76134a",
   "metadata": {},
   "source": [
    "##### Produce Standards Report for JES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74897e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JES Standards List write complete.\n",
      "73 files with 2036 standards processed in 6.854137799999997 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "jes_standards = get_standards_list(jes_cleaned)\n",
    "\n",
    "if len(jes_standards) > 0:\n",
    "    standards_report = parent_folder + '\\\\JES Standards Listing ' + '{:%Y%m%d %H%M%S}'.format(datetime.datetime.now()) + '.xlsx'\n",
    "\n",
    "    df = pd.DataFrame(jes_standards)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.to_excel(standards_report, header=['Section', \n",
    "                                          'Name', \n",
    "                                          'Date', \n",
    "                                          'Organization', \n",
    "                                          'Abbrev', \n",
    "                                          'Standard', \n",
    "                                          'Title', \n",
    "                                          'Edition'])\n",
    "\n",
    "    print('JES Standards List write complete.')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f'{len(jes_cleaned)} files with {len(jes_standards)} standards processed in {total_time} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d518a",
   "metadata": {},
   "source": [
    "##### Produce Standards Report for UFSG\n",
    "The following is expected to take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d081ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UFGS Standards List write complete.\n",
      "74 files with 18515 standards processed in 53.3476292 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "ufgs_standards = get_standards_list(ufgs_cleaned)\n",
    "\n",
    "if len(ufgs_standards) > 0:\n",
    "    standards_report = parent_folder + '\\\\UFGS Standards Listing ' + '{:%Y%m%d %H%M%S}'.format(datetime.datetime.now()) + '.xlsx'\n",
    "\n",
    "    df = pd.DataFrame(ufgs_standards)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    df.to_excel(standards_report, header=['Section', \n",
    "                                          'Name', \n",
    "                                          'Date', \n",
    "                                          'Organization', \n",
    "                                          'Abbrev', \n",
    "                                          'Standard', \n",
    "                                          'Title', \n",
    "                                          'Edition'])\n",
    "\n",
    "    print('UFGS Standards List write complete.')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f'{len(ufgs_cleaned)} files with {len(ufgs_standards)} standards processed in {total_time} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
